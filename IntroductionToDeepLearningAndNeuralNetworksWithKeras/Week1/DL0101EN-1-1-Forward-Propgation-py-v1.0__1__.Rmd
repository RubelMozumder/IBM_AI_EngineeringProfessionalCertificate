---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.13.6
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<a href="https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork20718188-2021-01-01"><img src = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png" width = 400> </a>

<h1 align=center><font size = 5>Artificial Neural Networks - Forward Propagation</font></h1>

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Introduction

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
In this lab, we will build a neural network from scratch and code how it performs predictions using forward propagation. Please note that all deep learning libraries have the entire training and prediction processes implemented, and so in practice you wouldn't really need to build a neural network from scratch. However, hopefully completing this lab will help you understand neural networks and how they work even better.

<!-- #endregion -->

<h2>Artificial Neural Networks - Forward Propagation</h2>

<h3>Objective for this Notebook<h3>    
<h5> 1. Initalize a Network</h5>
<h5> 2. Compute Weighted Sum at Each Node. </h5>
<h5> 3. Compute Node Activation </h5>
<h5> 4. Access your <b>Flask</b> app via a webpage anywhere using a custom link. </h5>     


<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Table of Contents

<div class="alert alert-block alert-info" style="margin-top: 20px">

<font size = 3>    

1.  <a href="https://#item11">Recap</a>
2.  <a href="https://#item12">Initalize a Network</a>
3.  <a href="https://#item13">Compute Weighted Sum at Each Node</a>
4.  <a href="https://#item14">Compute Node Activation</a>
5.  <a href="https://#item15">Forward Propagation</a>

</font>

</div>

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<a id="item1"></a>

<!-- #endregion -->

<a id='item11'></a>


<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
# Recap

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
From the videos, let's recap how a neural network makes predictions through the forward propagation process. Here is a neural network that takes two inputs, has one hidden layer with two nodes, and an output layer with one node.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<img src="http://cocl.us/neural_network_example" alt="Neural Network Example" width=600px>

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's start by randomly initializing the weights and the biases in the network. We have 6 weights and 3 biases, one for each node in the hidden layer as well as for each node in the output layer.

<!-- #endregion -->

```{python}
# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. 
# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.

# #!pip install numpy==1.21.4
```

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
import numpy as np # import Numpy library to generate 

weights = np.around(np.random.uniform(size=6), decimals=2) # initialize the weights
biases = np.around(np.random.uniform(size=3), decimals=2) # initialize the biases
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's print the weights and biases for sanity check.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
print(weights)
print(biases)
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Now that we have the weights and the biases defined for the network, let's compute the output for a given input, $x\_1$ and $x\_2$.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
x_1 = 0.5 # input 1
x_2 = 0.85 # input 2

print('x1 is {} and x2 is {}'.format(x_1, x_2))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's start by computing the wighted sum of the inputs, $z\_{1, 1}$, at the first node of the hidden layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]

print('The weighted sum of the inputs at the first node in the hidden layer is {}'.format(z_11))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Next, let's compute the weighted sum of the inputs, $z\_{1, 2}$, at the second node of the hidden layer. Assign the value to **z\_12**.
<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
z_12 = x_1*weights[2] + x_2*weights[3] + biases[1]

```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Print the weighted sum.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
print('The weighted sum of the inputs at the second node in the hidden layer is {}'.format(np.around(z_12, decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Next, assuming a sigmoid activation function, let's compute the activation of the first node, $a\_{1, 1}$, in the hidden layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
a_11 = 1.0 / (1.0 + np.exp(-z_11))

print('The activation of the first node in the hidden layer is {}'.format(np.around(a_11, decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's also compute the activation of the second node, $a\_{1, 2}$, in the hidden layer. Assign the value to **a\_12**.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
a_12 = 1.0 / (1.0 + np.exp(-z_12))
print('Activation of the second node in the heidden layer is {}'.format(np.around(a_12, decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
a_12 = 1.0 / (1.0 + np.exp(-z_12))
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Print the activation of the second node.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Now these activations will serve as the inputs to the output layer. So, let's compute the weighted sum of these inputs to the node in the output layer. Assign the value to **z\_2**.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
z_2 = a_11*weights[4] + a_12*weights[5] + biases[2]
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
z_2 = a_11 * weights[4] + a_12 * weights[5] + biases[2]
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Print the weighted sum of the inputs at the node in the output layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
print('The weighted sum of the inputs at the node in the output layer is {}'.format(np.around(z_2, decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Finally, let's compute the output of the network as the activation of the node in the output layer. Assign the value to **a\_2**.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
a_2 = 1/(1 + np.exp(-z_2))

```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
a_2 = 1.0 / (1.0 + np.exp(-z_2))
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Print the activation of the node in the output layer which is equivalent to the prediction made by the network.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
print('The output of the network for x1 = 0.5 and x2 = 0.85 is {}'.format(np.around(a_2, decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<hr>

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Obviously, neural networks for real problems are composed of many hidden layers and many more nodes in each layer. So, we can't continue making predictions using this very inefficient approach of computing the weighted sum at each node and the activation of each node manually.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
In order to code an automatic way of making predictions, let's generalize our network. A general network would take $n$ inputs, would have many hidden layers, each hidden layer having $m$ nodes, and would have an output layer. Although the network is showing one hidden layer, but we will code the network to have many hidden layers. Similarly, although the network shows an output layer with one node, we will code the network to have more than one node in the output layer.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<img src="http://cocl.us/general_neural_network" alt="Neural Network General" width=600px>

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<a id="item2"></a>

<!-- #endregion -->

<a id='item12'></a>


<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Initialize a Network

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's start by formally defining the structure of the network.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
n = 2 # number of inputs
num_hidden_layers = 2 # number of hidden layers
m = [2, 2] # number of nodes in each hidden layer
num_nodes_output = 1 # number of nodes in the output layer
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Now that we defined the structure of the network, let's go ahead and inititailize the weights and the biases in the network to random numbers. In order to be able to initialize the weights and the biases to random numbers, we will need to import the **Numpy** library.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
import numpy as np # import the Numpy library

num_nodes_previous = n # number of nodes in the previous layer

network = {} # initialize network an an empty dictionary

# loop through each layer and randomly initialize the weights and biases associated with each node
# notice how we are adding 1 to the number of hidden layers in order to include the output layer
for layer in range(num_hidden_layers + 1): 
    
    # determine name of layer
    if layer == num_hidden_layers:
        layer_name = 'output'
        num_nodes = num_nodes_output
    else:
        layer_name = 'layer_{}'.format(layer + 1)
        num_nodes = m[layer]
    
    # initialize weights and biases associated with each node in the current layer
    network[layer_name] = {}
    for node in range(num_nodes):
        node_name = 'node_{}'.format(node+1)
        network[layer_name][node_name] = {
            'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),
            'bias': np.around(np.random.uniform(size=1), decimals=2),
        }
    
    num_nodes_previous = num_nodes
    
print(network) # print network
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Awesome! So now with the above code, we are able to initialize the weights and the biases pertaining to any network of any number of hidden layers and number of nodes in each layer. But let's put this code in a function so that we are able to repetitively execute all this code whenever we want to construct a neural network.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
def initialize_network(num_inputs, num_hidden_layers, num_nodes_hidden, num_nodes_output):
    
    num_nodes_previous = num_inputs # number of nodes in the previous layer

    network = {}
    
    # loop through each layer and randomly initialize the weights and biases associated with each layer
    for layer in range(num_hidden_layers + 1):
        
        if layer == num_hidden_layers:
            layer_name = 'output' # name last layer in the network output
            num_nodes = num_nodes_output
        else:
            layer_name = 'layer_{}'.format(layer + 1) # otherwise give the layer a number
            num_nodes = num_nodes_hidden[layer] 
        
        # initialize weights and bias for each node
        network[layer_name] = {}
        for node in range(num_nodes):
            node_name = 'node_{}'.format(node+1)
            network[layer_name][node_name] = {
                'weights': np.around(np.random.uniform(size=num_nodes_previous), decimals=2),
                'bias': np.around(np.random.uniform(size=1), decimals=2),
            }
    
        num_nodes_previous = num_nodes

    return network # return the network
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
#### Use the *initialize_network* function to create a network that:

1.  takes 5 inputs
2.  has three hidden layers
3.  has 3 nodes in the first layer, 2 nodes in the second layer, and 3 nodes in the third layer
4.  has 1 node in the output layer

Call the network **small_network**.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
num_inputs = 5
num_hidden_layers = 3
num_nodes_hidden = [3,2,3]
num_nodes_output = 1

SmallNetwork = initialize_network(num_inputs=num_inputs, 
                   num_hidden_layers=num_hidden_layers,
                   num_nodes_hidden=num_nodes_hidden,
                   num_nodes_output=num_nodes_output)
SmallNetwork
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
small_network = initialize_network(5, 3, [3, 2, 3], 1)
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Compute Weighted Sum at Each Node

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
The weighted sum at each node is computed as the dot product of the inputs and the weights plus the bias. So let's create a function called *compute_weighted_sum* that does just that.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
def compute_weighted_sum(inputs, weights, bias):
    return np.sum(inputs * weights) + bias
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Let's generate 5 inputs that we can feed to **small_network**.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
from random import seed
import numpy as np

np.random.seed(12)
inputs = np.around(np.random.uniform(size=num_inputs), decimals=2)

print('The inputs to the network are {}'.format(inputs))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
#### Use the *compute_weighted_sum* function to compute the weighted sum at the first node in the first hidden layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here
weights = Network['layer_1']['node_1']['weights']
bias = Network['layer_1']['node_1']['bias']
weighted_sum = compute_weighted_sum(inputs, weights, bias)
print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
node_weights = small_network['layer_1']['node_1']['weights']
node_bias = small_network['layer_1']['node_1']['bias']

weighted_sum = compute_weighted_sum(inputs, node_weights, node_bias)
print('The weighted sum at the first node in the hidden layer is {}'.format(np.around(weighted_sum[0], decimals=4)))
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Compute Node Activation

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Recall that the output of each node is simply a non-linear tranformation of the weighted sum. We use activation functions for this mapping. Let's use the sigmoid function as the activation function here. So let's define a function that takes a weighted sum as input and returns the non-linear transformation of the input using the sigmoid function.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
def node_activation(weighted_sum):
    return 1.0 / (1.0 + np.exp(-1 * weighted_sum))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
#### Use the *node_activation* function to compute the output of the first node in the first hidden layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answer here

node_activation(weighted_sum)

```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
node_output  = node_activation(compute_weighted_sum(inputs, node_weights, node_bias))
print('The output of the first node in the hidden layer is {}'.format(np.around(node_output[0], decimals=4)))
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<a id="item5"></a>

<!-- #endregion -->

<a id='item15'></a>


<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
## Forward Propagation

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
The final piece of building a neural network that can perform predictions is to put everything together. So let's create a function that applies the *compute_weighted_sum* and *node_activation* functions to each node in the network and propagates the data all the way to the output layer and outputs a prediction for each node in the output layer.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
The way we are going to accomplish this is through the following procedure:

1.  Start with the input layer as the input to the first hidden layer.
2.  Compute the weighted sum at the nodes of the current layer.
3.  Compute the output of the nodes of the current layer.
4.  Set the output of the current layer to be the input to the next layer.
5.  Move to the next layer in the network.
6.  Repeat steps 2 - 4 until we compute the output of the output layer.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
def forward_propagate(network, inputs):
    
    layer_inputs = list(inputs) # start with the input layer as the input to the first hidden layer
    
    for layer in network:
        
        layer_data = network[layer]
        
        layer_outputs = [] 
        for layer_node in layer_data:
        
            node_data = layer_data[layer_node]
        
            # compute the weighted sum and the output of each node at the same time 
            node_output = node_activation(compute_weighted_sum(layer_inputs, node_data['weights'], node_data['bias']))
            layer_outputs.append(np.around(node_output[0], decimals=4))
            
        if layer != 'output':
            print('The outputs of the nodes in hidden layer number {} is {}'.format(layer.split('_')[1], layer_outputs))
    
        layer_inputs = layer_outputs # set the output of this layer to be the input to next layer

    network_predictions = layer_outputs
    return network_predictions
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
#### Use the *forward_propagate* function to compute the prediction of our small network

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### type your answser here

output = forward_propagate(SmallNetwork, inputs)
output[0]
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Double-click **here** for the solution.

<!-- The correct answer is:
predictions = forward_propagate(small_network, inputs)
print('The predicted value by the network for the given input is {}'.format(np.around(predictions[0], decimals=4)))
-->

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
So we built the code to define a neural network. We can specify the number of inputs that a neural network can take, the number of hidden layers as well as the number of nodes in each hidden layer, and the number of nodes in the output layer.

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
We first use the *initialize_network* to create our neural network and define its weights and biases.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
my_network = initialize_network(num_inputs=5, 
                                num_hidden_layers=3, 
                                num_nodes_hidden=[2, 3, 2], 
                                num_nodes_output=3)
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Then, for a given input,

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
inputs = np.around(np.random.uniform(size=5), decimals=2)
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
we compute the network predictions.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
predictions = forward_propagate(my_network, inputs)
print('The predicted values by the network for the given input are {}'.format(predictions))
```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
Feel free to play around with the code by creating different networks of different structures and enjoy making predictions using the *forward_propagate* function.

<!-- #endregion -->

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### create a network

class Node(object):
    """Node for layer."""
    
    def __init__(self, weights=[], 
                 bias=None):
        """ Initialise node with empty bias, weights and output."""
          
        import numpy as np
        self.weights = weights
        self.bias = bias
        self.output = None
        self.inputs = None
        
    def fill_parameters(self, bias=None, weights=None):
        
        if bias:
            self.bias.append(bias)
            
        if weights:
            if isinstance(self.weights, list):
                self.weights = weights
            else:
                self.weights.append(weights)
                
    def output(self, inputs, activation='softmax'):
        
        self.inputs = inputs
        out = np.sum(self.weights * self.inputs)
        
        if activation=='softmax':
            out = self.softmax(out)
            
        self.output = out
    
    def softmax(self, z):
        return 1/(1+ np.exp(-z))
    
    def __str__(self):
        return(f'inputs: {self.inputs}, weihts: {self.weights} bias: {self.bias},'
               f'out :{self.output}')

class Layer(Node):
    """Construct layer with the number of nodes."""
    

    def __init__(self, node_num, layer_index):
        """Construct layer with def init number of nodes."""
        self.node_num = node_num
        self.nodes_dict = dict()
        self.node_list = []
        self.layer_index = layer_index

        
    # node generator    
    def nodes(self):

        if not self.node_list:
            for i in range(self.node_num):
                empty_node = Node()
                self.node_list.append(empty_node)
                self.nodes_dict[len(self.node_list)] = empty_node
        return iter(self.node_list)
    
    def add_node(self, node_index, weights, bias):
        if len(self.nodes_dict.keys())<self.node_num:
            self.nodes_dict[node_index] = Node(weights=weights,
                                          bias=bias)
            if len(self.nodes_dict.keys())==self.node_num:
                for key in (self.nodes_dict.keys().sort()):
                    node_list.append(nodes_dict[key])
        else:
            raise ('Maximum layer node is completed, stop adding further nodes in this layer.')
    
    
    def __str__(self):
        nodes_tuple = []
        
        i=0
        for key in iter(self.nodes_dict.keys()):
            i = i+1
            node_ = i
            nodes_tuple.append(('node_'+str(i)+':', self.nodes_dict[key].__str__()))
        return f'layer {self.layer_index} : {tuple(nodes_tuple)}, '

    
class Network(Layer):

    
    def __init__(self, h_layer, layer_nodes, out_nodes):
    
        self.h_layer = h_layer
        self.out_nodes = out_nodes
        self.layer_nodes = layer_nodes
        self.layer_dict = dict()
    
    
    def construct_input_layer(self, inputs, nodes_num):
        layer_0 = Layer(node_num=nodes_num,
                                   layer_index=0)
        
        inputs_it = iter(inputs)
        for node in layer_0.nodes():
            inp = next(inputs_it)
            node.output = inp
            node.inputs = inp
                    
        self.layer_dict[0] = layer_0
    
    
    def construct_h_layer(self, layer_ind,
                          node_num):
        layer_n = Layer(node_num=node_num,
                        layer_index=layer_ind)
    
    
    @property
    def inputs(self):
        return print(self.layer_dict[0])
    
#    def construct_layer(self, layer_index)
        
        
    



```

```{python}
layer_nodes_num=4
weights = np.random.uniform(size=layer_nodes_num)
bais = np.random.uniform(size=1)
#node_1 = Node(weights=weights, bias=bias)
#print(node_1)

layer1 = layer(node_num=layer_nodes_num, layer_index=1)
layer1.add_node(node_index=1,weights=weights, bias=bias)

print(layer1)
```

```{python}
Network1 = Network(h_layer=3, layer_nodes=[4,4,4], out_nodes=1)

Network1.construct_input_layer(inputs=weights, nodes_num=4)
```

```{python}
Network1.inputs
```

```{python button=FALSE, jupyter={'outputs_hidden': True}, new_sheet=FALSE, run_control={'read_only': False}}
### create another network




```

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
### Thank you for completing this lab!

This notebook was created by [Alex Aklson](https://www.linkedin.com/in/aklson/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork20718188-2021-01-01). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!

<!-- #endregion -->

## Change Log

| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |
| ----------------- | ------- | ---------- | ----------------------------------------------------------- |
| 2020-09-21        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |

<hr>

## <h3 align="center"> © IBM Corporation 2020. All rights reserved. <h3/>


<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
This notebook is part of a course on **Coursera** called *Introduction to Deep Learning & Neural Networks with Keras*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0101EN_Coursera_Week1\_LAB1).

<!-- #endregion -->

<!-- #region button=false new_sheet=false run_control={"read_only": false} -->
<hr>

Copyright © 2019 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork20718188-2021-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork20718188-2021-01-01).

<!-- #endregion -->
